{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "id": "b0d41641-8f2d-43b6-81b4-dd059d53a46c",
   "cell_type": "markdown",
   "source": "<h1>Task 1 - DL</h1>",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<h3>Step 1: Data Preprocessing</h3>",
   "id": "31ba8fd1-7215-4cf3-bb11-0963f7893377"
  },
  {
   "id": "99d963fc-8d8a-4bad-a19f-08c26307b07c",
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:38:37.542266Z",
     "start_time": "2025-04-12T12:38:37.536750Z"
    }
   },
   "outputs": [],
   "execution_count": 11
  },
  {
   "id": "01a54bc0-30bd-451a-9c93-a8326fecc1f3",
   "cell_type": "code",
   "source": [
    "class ArtifactDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, file_name)\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Extract the label from the filename\n",
    "        label = int(file_name.split('_')[-1].split('.')[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def get_transforms(phase='train'):\n",
    "    if phase == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:38:39.894658Z",
     "start_time": "2025-04-12T12:38:39.883732Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "id": "515b88e0-442d-4a36-9aef-2b22574b3bb7",
   "cell_type": "code",
   "source": [
    "train_dir = \"dataset/train\"\n",
    "val_dir = \"dataset/test\"\n",
    "\n",
    "train_dataset = ArtifactDataset(train_dir, transform=get_transforms('train'))\n",
    "val_dataset = ArtifactDataset(val_dir, transform=get_transforms('test'))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:44:16.992369Z",
     "start_time": "2025-04-12T12:44:16.972880Z"
    }
   },
   "outputs": [],
   "execution_count": 26
  },
  {
   "id": "87acc289-e492-4392-b1a4-0d65bc441862",
   "cell_type": "markdown",
   "source": "<h3>Step 2: Model Initialization and Fine-Tuning</h3>",
   "metadata": {}
  },
  {
   "id": "e64f0ea2-7530-417d-b1e0-af70e74d2b84",
   "cell_type": "code",
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:41:57.484056Z",
     "start_time": "2025-04-12T12:41:57.465501Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "id": "f02e5f17-e28c-4b7d-8865-aa0d77a60b42",
   "cell_type": "code",
   "source": [
    "# For using resnet \n",
    "# model = models.resnet18(weights='DEFAULT')\n",
    "# Load the ResNet-18 model architecture from torchvision\n",
    "# model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "# Replace the final fully-connected layer with one matching the number of classes\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "model._fc = nn.Linear(model._fc.in_features, 2) #for binary classification\n",
    "    \n",
    "model = model.to(device)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:42:31.809989Z",
     "start_time": "2025-04-12T12:42:31.620543Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "id": "2cfb4b87-965c-4da7-8683-657dd85e6cff",
   "cell_type": "markdown",
   "source": "<h3>Step 3: Model Training and Validation</h3>",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T12:43:18.858904Z",
     "start_time": "2025-04-12T12:43:13.164539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score"
   ],
   "id": "b8a2af546155884d",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:43:19.782344Z",
     "start_time": "2025-04-12T12:43:19.772108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc='Train'):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    return running_loss / len(dataloader), f1\n",
    "\n"
   ],
   "id": "ffe6214d-3402-4f36-98b2-c36c393716ce",
   "outputs": [],
   "execution_count": 23
  },
  {
   "id": "50f3c7f7-a7f7-4626-bbab-06a79a891570",
   "cell_type": "code",
   "source": "def validate(model, dataloader, criterion, device):\n    model.eval()\n    val_loss = 0.0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for images, labels in tqdm(dataloader, desc='Validate'):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            preds = torch.argmax(outputs, dim=1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    f1 = f1_score(all_labels, all_preds, average='micro')\n    return val_loss / len(dataloader), f1",
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:43:30.015213Z",
     "start_time": "2025-04-12T12:43:30.004962Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "id": "063d289e-689b-40b4-bad8-1438a5dc93de",
   "cell_type": "markdown",
   "source": "<h3>Step 4: Model Training and Validation</h3>",
   "metadata": {}
  },
  {
   "id": "5c8cda0a-d9bd-4aeb-9afb-226a793023f6",
   "cell_type": "code",
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_f1 = 0.0\n",
    "\n",
    "train_f1_scores = []\n",
    "val_f1_scores = []\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nüîÅ Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    train_loss, train_f1 = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_f1 = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    train_f1_scores.append(train_f1)\n",
    "    val_f1_scores.append(val_f1)\n",
    "\n",
    "    print(f\"üìà Train Loss: {train_loss:.4f} | F1: {train_f1:.4f}\")\n",
    "    print(f\"üß™ Val Loss:   {val_loss:.4f} | F1: {val_f1:.4f}\")\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'efficientnet_best_model.pt')\n",
    "        print(\"‚úÖ Model saved!\")\n",
    "\n",
    "print(f\"\\nüèÅ Best Val F1: {best_f1:.4f}\")"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T12:50:06.566629Z",
     "start_time": "2025-04-12T12:50:00.061663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/57 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[30]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m      8\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33müîÅ Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m     train_loss, train_f1 = \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m     val_loss, val_f1 = validate(model, val_loader, criterion, device)\n\u001B[32m     14\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33müìà Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_f1\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[23]\u001B[39m\u001B[32m, line 10\u001B[39m, in \u001B[36mtrain_one_epoch\u001B[39m\u001B[34m(model, dataloader, optimizer, criterion, device)\u001B[39m\n\u001B[32m      7\u001B[39m images, labels = images.to(device), labels.to(device)\n\u001B[32m      9\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     12\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficientnet_pytorch\\model.py:314\u001B[39m, in \u001B[36mEfficientNet.forward\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    304\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"EfficientNet's forward function.\u001B[39;00m\n\u001B[32m    305\u001B[39m \u001B[33;03m   Calls extract_features to extract features, applies final linear layer, and returns logits.\u001B[39;00m\n\u001B[32m    306\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    311\u001B[39m \u001B[33;03m    Output of this model after processing.\u001B[39;00m\n\u001B[32m    312\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    313\u001B[39m \u001B[38;5;66;03m# Convolution layers\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m314\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mextract_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    315\u001B[39m \u001B[38;5;66;03m# Pooling and final linear layer\u001B[39;00m\n\u001B[32m    316\u001B[39m x = \u001B[38;5;28mself\u001B[39m._avg_pooling(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficientnet_pytorch\\model.py:296\u001B[39m, in \u001B[36mEfficientNet.extract_features\u001B[39m\u001B[34m(self, inputs)\u001B[39m\n\u001B[32m    294\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m drop_connect_rate:\n\u001B[32m    295\u001B[39m         drop_connect_rate *= \u001B[38;5;28mfloat\u001B[39m(idx) / \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._blocks)  \u001B[38;5;66;03m# scale drop connect_rate\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m296\u001B[39m     x = \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdrop_connect_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[38;5;66;03m# Head\u001B[39;00m\n\u001B[32m    299\u001B[39m x = \u001B[38;5;28mself\u001B[39m._swish(\u001B[38;5;28mself\u001B[39m._bn1(\u001B[38;5;28mself\u001B[39m._conv_head(x)))\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficientnet_pytorch\\model.py:109\u001B[39m, in \u001B[36mMBConvBlock.forward\u001B[39m\u001B[34m(self, inputs, drop_connect_rate)\u001B[39m\n\u001B[32m    106\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._bn0(x)\n\u001B[32m    107\u001B[39m     x = \u001B[38;5;28mself\u001B[39m._swish(x)\n\u001B[32m--> \u001B[39m\u001B[32m109\u001B[39m x = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_depthwise_conv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    110\u001B[39m x = \u001B[38;5;28mself\u001B[39m._bn1(x)\n\u001B[32m    111\u001B[39m x = \u001B[38;5;28mself\u001B[39m._swish(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\efficientnet_pytorch\\utils.py:275\u001B[39m, in \u001B[36mConv2dStaticSamePadding.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    273\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m    274\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.static_padding(x)\n\u001B[32m--> \u001B[39m\u001B[32m275\u001B[39m     x = \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    276\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T14:24:01.072146Z",
     "start_time": "2025-04-12T14:24:00.018418Z"
    }
   },
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt",
   "id": "97cadf051567688e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(range(1, epochs + 1), train_f1_scores, label='Train F1', color='blue')\n",
    "plt.plot(range(1, epochs + 1), val_f1_scores, label='Val F1', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score (Micro)')\n",
    "plt.title('Training and Validation F1 Score over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "ecf5235e7c9d4016"
  },
  {
   "id": "6abcd638-412b-45e2-91b2-6a05326cd6ee",
   "cell_type": "markdown",
   "source": "",
   "metadata": {}
  }
 ]
}
